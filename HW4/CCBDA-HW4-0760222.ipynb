{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#df = pd.read_csv('public.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Pyspark to view dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CustomerId: integer (nullable = true)\n",
      " |-- Surname: string (nullable = true)\n",
      " |-- CreditScore: integer (nullable = true)\n",
      " |-- Geography: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Tenure: integer (nullable = true)\n",
      " |-- Balance: double (nullable = true)\n",
      " |-- NumOfProducts: integer (nullable = true)\n",
      " |-- HasCrCard: integer (nullable = true)\n",
      " |-- IsActiveMember: integer (nullable = true)\n",
      " |-- EstimatedSalary: double (nullable = true)\n",
      " |-- Exited: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# These part is  for windows version, if you use ubuntu, remember to edit import pyspark part\n",
    "# ----\n",
    "import findspark\n",
    "\n",
    "findspark.init('/home/austin/spark-2.3.3-bin-hadoop2.7')\n",
    "findspark.find()\n",
    "import pyspark\n",
    "findspark.find()\n",
    "# ----\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Churn_Modelling\").getOrCreate()\n",
    "df = spark.read.csv('public.csv',header=True,inferSchema=True)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do your work here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|Exited|count|\n",
      "+------+-----+\n",
      "|     1| 1644|\n",
      "|     0| 6356|\n",
      "+------+-----+\n",
      "\n",
      "+------+-----+\n",
      "|Exited|count|\n",
      "+------+-----+\n",
      "|     1| 4932|\n",
      "|     0| 6356|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols = df.columns\n",
    "df.groupby(\"Exited\").count().show()\n",
    "#df.groupby(\"Exited\").mean().show()\n",
    "df_one = df.filter(df['Exited'] == 1)\n",
    "for i in range(2):\n",
    "    df = df.union(df_one)\n",
    "df.groupby(\"Exited\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Normalizer\n",
    "\n",
    "# Normalize each Vector using $L^1$ norm.\n",
    "#normalizer = Normalizer(inputCol=\"CreditScore\", outputCol=\"normCreditScore\", p=2.0)\n",
    "#l1NormData.show('CreditScore')\n",
    "# Normalize each Vector using $L^\\infty$ norm.\n",
    "#lInfNormData = normalizer.transform(df, {normalizer.p: float(\"inf\")})\n",
    "#print(l1NormData)\n",
    "def normolize(df):\n",
    "    balance_max = df.select(\"Balance\").describe().collect()[4].asDict()[\"Balance\"]\n",
    "    credit_max = df.select(\"CreditScore\").describe().collect()[4].asDict()[\"CreditScore\"]\n",
    "    credit_min = df.select(\"CreditScore\").describe().collect()[3].asDict()[\"CreditScore\"]\n",
    "    salary_max = df.select(\"EstimatedSalary\").describe().collect()[4].asDict()[\"EstimatedSalary\"]\n",
    "    salary_min = df.select(\"EstimatedSalary\").describe().collect()[3].asDict()[\"EstimatedSalary\"]\n",
    "    df = df.withColumn(\"Nor_Balance\",df[\"Balance\"] / balance_max)\n",
    "    df = df.withColumn(\"Nor_CreditScore\",(df[\"CreditScore\"] - credit_min) / credit_max)\n",
    "    df = df.withColumn(\"Nor_EstimatedSalary\",(df[\"EstimatedSalary\"] - salary_min) / salary_max)\n",
    "    return df\n",
    "df= normolize(df)\n",
    "#df.select(\"Nor_Balance\",\"Nor_CreditScore\",\"Nor_EstimatedSalary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "categoricalColumns = [\"Geography\", \"Gender\"]\n",
    "stages = []\n",
    "for categoricalCol in categoricalColumns:\n",
    "    # Category Indexing with StringIndexer\n",
    "    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n",
    "    # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n",
    "    # encoder = OneHotEncoderEstimator(inputCol=categoricalCol + \"Index\", outputCol=categoricalCol + \"classVec\")\n",
    "    encoder = OneHotEncoder(inputCol=stringIndexer.getOutputCol(), outputCol=categoricalCol + \"classVec\", dropLast = False)\n",
    "    # Add stages.  These are not run here, but will run all at once later on.\n",
    "    stages += [stringIndexer, encoder]\n",
    "\n",
    "#m = stages[2].fit(df)\n",
    "#dft = m.transform(df)\n",
    "#stages[3].transform(dft)[\"GenderclassVec\",\"GenderIndex\"].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "numericCols = [\"Age\",\"Tenure\",\"Nor_CreditScore\",\"Nor_Balance\", \"NumOfProducts\", \"HasCrCard\", \"IsActiveMember\",\"Nor_EstimatedSalary\"]\n",
    "assemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[features: vector, CustomerId: int, Surname: string, CreditScore: int, Geography: string, Gender: string, Age: int, Tenure: int, Balance: double, NumOfProducts: int, HasCrCard: int, IsActiveMember: int, EstimatedSalary: double, Exited: int]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7952\n",
      "3336\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "partialPipeline = Pipeline().setStages(stages)\n",
    "pipelineModel = partialPipeline.fit(df)\n",
    "preppedDataDF = pipelineModel.transform(df)\n",
    "\n",
    "#display(lrModel, preppedDataDF, \"ROC\")\n",
    "# Keep relevant columns\n",
    "selectedcols = [\"features\"] + cols\n",
    "dataset = preppedDataDF.select(selectedcols)\n",
    "display(dataset)\n",
    "\n",
    "(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed=100)\n",
    "print(trainingData.count())\n",
    "print(testData.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7738780186182422"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create initial LogisticRegression model\n",
    "lr = LogisticRegression(labelCol=\"Exited\", featuresCol=\"features\", maxIter=10)\n",
    "\n",
    "# Train model with Training Data\n",
    "lrModel = lr.fit(trainingData)\n",
    "\n",
    "predictions = lrModel.transform(testData)\n",
    "# View model's predictions and probabilities of each prediction class\n",
    "# You can select any columns in the above schema to view as well. For example's sake we will choose age & occupation\n",
    "selected = predictions.select(\"Exited\", \"prediction\", \"probability\", \"EstimatedSalary\", \"AGE\")\n",
    "#selected.show(100)\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Evaluate model\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"Exited\",rawPredictionCol=\"rawPrediction\")\n",
    "evaluator.evaluate(predictions)\n",
    "#trainingSummary = lrModel.summary\n",
    "\n",
    "#testData.groupby(\"Exited\").count().show()\n",
    "#predictions.groupby(\"prediction\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8617877415586483"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# Create an initial RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol=\"Exited\", featuresCol=\"features\")\n",
    "\n",
    "# Train model with Training Data\n",
    "rfModel = rf.fit(trainingData)\n",
    "# Make predictions on test data using the Transformer.transform() method.\n",
    "predictions = rfModel.transform(testData)\n",
    "#predictions.printSchema()\n",
    "# View model's predictions and probabilities of each prediction class\n",
    "#selected = predictions.select(\"Exited\", \"prediction\", \"probability\")\n",
    "#display(selected)\n",
    "#We will evaluate our Random Forest model with BinaryClassificationEvaluator.\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Evaluate model\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"Exited\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8813351351827239"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create ParamGrid for Cross Validation\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(rf.maxDepth, [2, 4, 6])\n",
    "             .addGrid(rf.maxBins, [20, 60])\n",
    "             .addGrid(rf.numTrees, [5, 20])\n",
    "             .build())\n",
    "# Create 5-fold CrossValidator\n",
    "cv = CrossValidator(estimator=rf, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n",
    "\n",
    "# Run cross validations.  This can take about 6 minutes since it is training over 20 trees!\n",
    "cvModel = cv.fit(trainingData)\n",
    "# Use test set here so we can measure the accuracy of our model on new data\n",
    "predictions = cvModel.transform(testData)\n",
    "# cvModel uses the best model found from the Cross Validation\n",
    "# Evaluate best model\n",
    "evaluator.evaluate(predictions)\n",
    "# View Best model's predictions and probabilities of each prediction class\n",
    "#selected = predictions.select(\"label\", \"prediction\", \"probability\", \"age\", \"occupation\")\n",
    "#display(selected)\n",
    "\n",
    "bestModel = cvModel.bestModel\n",
    "# Generate predictions for entire dataset\n",
    "finalPredictions = bestModel.transform(dataset)\n",
    "# Evaluate best model\n",
    "evaluator.evaluate(finalPredictions)\n",
    "\n",
    "#trainingSummary = lrModel.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7569270889927197\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7601744186046513"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pr = trainingSummary.pr.toPandas()\n",
    "'''\n",
    "plt.plot(pr['recall'],pr['precision'])\n",
    "plt.ylabel('Precision')\n",
    "plt.xlabel('Recall')\n",
    "'''\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "data_array =  np.array(dataset.select('Exited').collect())\n",
    "prediction_array = np.array(finalPredictions.select('prediction').collect())\n",
    "print(metrics.f1_score(data_array,prediction_array))\n",
    "\n",
    "data_array =  np.array(testData.select('Exited').collect())\n",
    "prediction_array = np.array(predictions.select('prediction').collect())\n",
    "metrics.f1_score(data_array,prediction_array) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load private dataset, the same structure as public dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_private = spark.read.csv('public.csv',header=True,inferSchema=True)  # TA takes public dataset as example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do prediction with your PySpark model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_private = normolize(df_private)\n",
    "pipelineModel = partialPipeline.fit(df_private)\n",
    "preppedData_private = pipelineModel.transform(df_private)\n",
    "selectedcols = [\"features\"] + cols\n",
    "dataset_private = preppedData_private.select(selectedcols)\n",
    "finalPredictions = bestModel.transform(dataset_private)\n",
    "finalPredictions = finalPredictions.drop('Exited')\n",
    "finalPredictions = finalPredictions.withColumnRenamed('prediction','Exited')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print Your result as the following type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|CustomerId|Exited|\n",
      "+----------+------+\n",
      "|  15565701|   0.0|\n",
      "|  15565706|   0.0|\n",
      "|  15565796|   1.0|\n",
      "|  15565806|   0.0|\n",
      "|  15565878|   0.0|\n",
      "+----------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "finalPredictions.select('CustomerId','Exited').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TA will use the following function to get your prediction result (f-1 score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6462566100751462"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "data_array =  np.array(df_private.select('Exited').collect())\n",
    "prediction_array = np.array(finalPredictions.select('Exited').collect())\n",
    "metrics.f1_score(data_array,prediction_array)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
